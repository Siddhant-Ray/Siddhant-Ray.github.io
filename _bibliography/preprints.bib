@misc{ray2024swiftqueueoptimizinglowlatencyapplications,
      title={{S}wift{Q}ueue: {O}ptimizing {L}ow-{L}atency {A}pplications with {S}wift {P}acket {Q}ueuing}, 
      author={Siddhant Ray and Xi Jiang and Jack Luo and Nick Feamster and Junchen Jiang},
      year={2024},
      eprint={2410.06112},
      archivePrefix={arXiv},
      primaryClass={cs.NI},
      note={In Submission},
      url={https://arxiv.org/abs/2410.06112}, 
      abbr={arXiv},
      abstract={Low Latency, Low Loss, and Scalable Throughput (L4S), as an emerging router-queue management technique, has seen steady deployment in the industry. An L4S-enabled router assigns each packet to the queue based on the packet header marking. Currently, L4S employs per-flow queue selection, i.e. all packets of a flow are marked the same way and thus use the same queues, even though each packet is marked separately. However, this may hurt tail latency and latency-sensitive applications because transient congestion and queue buildups may only affect a fraction of packets in a flow.
                We present SwiftQueue, a new L4S queue-selection strategy in which a sender uses a novel per-packet latency predictor to pinpoint which packets likely have latency spikes or drops. The insight is that many packet-level latency variations result from complex interactions among recent packets at shared router queues. Yet, these intricate packet-level latency patterns are hard to learn efficiently by traditional models. Instead, SwiftQueue uses a custom Transformer, which is well-studied for its expressiveness on sequential patterns, to predict the next packet's latency based on the latencies of recently received ACKs. Based on the predicted latency of each outgoing packet, SwiftQueue's sender dynamically marks the L4S packet header to assign packets to potentially different queues, even within the same flow. Using real network traces, we show that SwiftQueue is 45-65% more accurate in predicting latency and its variations than state-of-art methods. Based on its latency prediction, SwiftQueue reduces the tail latency for L4S-enabled flows by 36-45%, compared with the existing L4S queue-selection method.},
    bibtex_show={true},
    arxiv={2410.06112},
}

@misc{yao2024cacheblendfastlargelanguage,
      title={Cache{B}lend: {F}ast {L}arge {L}anguage {M}odel {S}erving for {R}{A}{G} with {C}ached {K}nowledge {F}usion}, 
      author={Jiayi Yao and Hanchen Li and Yuhan Liu and Siddhant Ray and Yihua Cheng and Qizheng Zhang and Kuntai Du and Shan Lu and Junchen Jiang},
      year={2024},
      eprint={2405.16444},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      note={To appear at EuroSys 2025},
      url={https://arxiv.org/abs/2405.16444}, 
      bibtex_show={true},
      abbr={arXiv},
      arxiv={2405.16444},
      abstract={Large language models (LLMs) often incorporate multiple text chunks in their inputs to provide 
      the necessary contexts. To speed up the prefill of the long LLM inputs, one can pre-compute the KV cache 
      of a text and re-use the KV cache when the context is reused as the prefix of another LLM input. However,
       the reused text chunks are not always the input prefix, and when they are not, their precomputed KV caches
        cannot be directly used since they ignore the text's cross-attention with the preceding text in the LLM 
        input. Thus, the benefits of reusing KV caches remain largely unrealized. This paper tackles just one 
        question: when an LLM input contains multiple text chunks, how to quickly combine their precomputed KV 
        caches in order to achieve the same generation quality as the expensive full prefill (i.e., without reusing
         KV cache)? We present CacheBlend, a scheme that reuses the pre-computed KV caches, regardless prefix or not,
          and selectively recomputes the KV values of a small subset of tokens to partially update each reused KV 
          cache. In the meantime,the small extra delay for recomputing some tokens can be pipelined with the retrieval
       of KV caches within the same job,allowing CacheBlend to store KV caches in slower devices with more storage 
       capacity while retrieving them without increasing the inference delay. By comparing CacheBlend with the 
       state-of-the-art KV cache reusing schemes on three open-source LLMs of various sizes and four popular
        benchmark datasets of different tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by 2.2-3.3X
         and increases the inference throughput by 2.8-5X, compared with full KV recompute, without compromising 
         generation quality or incurring more storage cost.},
}

@misc{ray2024ragservefastqualityawarerag,
      title={{RAGS}erve: {F}ast {Q}uality-{A}ware {RAG} {S}ystems with {C}onfiguration {A}daptation}, 
      author={Siddhant Ray and Rui Pan and Zhuohan Gu and Kuntai Du and Ganesh Ananthanarayanan and Ravi Netravali and Junchen Jiang},
      year={2024},
      eprint={2412.10543},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2412.10543}, 
      bibtex_show={true},
      abbr={arXiv},
      arxiv={2412.10543},
      abstract={RAG (Retrieval Augmented Generation) allows LLMs (large language models) to generate better responses with external knowledge, but using more external knowledge often improves 
          generation quality at the expense of response delay. Prior work either reduces the response delay (through better scheduling of RAG queries) or strives to maximize quality (which involves tuning the RAG workflow),
          but they fall short in optimizing the tradeoff between the delay and quality of RAG responses.
          This paper presents RAGServe, the first RAG system that jointly schedules queries and adapts 
          the key RAG configurations of each query, such as the number of retrieved text chunks and 
          synthesis methods, in order to balance quality optimization and response delay reduction. 
          Using 4 popular RAG-QA datasets, we show that compared with the state-of-the-art RAG optimization 
          schemes, RAGServe reduces the generation latency by 1.64-2.54X without sacrificing generation quality.},
}